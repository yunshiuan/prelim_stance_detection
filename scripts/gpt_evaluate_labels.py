"""
Evaluate the labels generated by GPT.
@author: Yun-Shiuan Chuang
@contact: yunshiuan.chuang@gmail.com
"""
from os.path import join, dirname
import pandas as pd
import numpy as np
import re
from sklearn.metrics import confusion_matrix

from utils import compute_metrics_without_trainer, creat_dir_for_a_file_if_not_exists, process_dataframe, partition_and_resample_df, convert_time_unit_into_name, get_parameters_for_dataset, add_version_suffix_to_file_name, get_dir_of_a_file, save_dict_as_yaml, AttrDict
from gpt_data_processor import SemEvalGPTDataProcessor
from data_processor import SemEvalDataProcessor


def main(config):
    """Evaluate the labels generated by GPT.

    Args:
        config (argparse.Namespace): the configuration of the experiment.
    """
    par = get_parameters_for_dataset(config.dataset)
    # convert the config into an AttrDict to access keys as attributes
    config = AttrDict(config)


    # ----------
    # SEM_EVAL
    # ----------
    if config.dataset_meta == "SEM_EVAL":
        gpt_data_processor = SemEvalGPTDataProcessor(topic=config.topic, version_prompt=par.VERSION_PROMPT)
        data_processor = SemEvalDataProcessor()
        # - data with ground-trueth labels for the evaluation
        df = process_dataframe(
            input_csv=data_processor._get_file_processed_default(topic=config.topic),
            dataset=config.dataset)
        df_partitions = data_processor.read_partitions(topic=config.topic)
        file_input_predictions = gpt_data_processor._get_file_predictions_default(topic=config.topic)
    else:
        raise ValueError("DATASET_META = {} is not supported.".format(config.dataset_meta))

    # - gda split
    if "gda" in config.list_eval_modes:
        dict_df_gda = partition_and_resample_df(
            df, seed=None, partition_type="gda",
            list_time_domain=config.dict_times[config.time_domain_unit][config.dataset],
            list_source_domain=config.dict_source_domain[config.time_domain_unit][config.dataset],
            partition_source_method=["train", "vali", "test"],
            partition_target_method=["all", "vali", "test"],
            read_partition_from_df=True,
            df_partitions=df_partitions)
        if not config.full_predictions:
            # remove "source_train_raw" as it was not predicted
            del dict_df_gda["source_train_raw"]
        # - single domain split
    elif "single_domain" in config.list_eval_modes:
        dict_df_single_domain = partition_and_resample_df(
            df, seed=None, partition_type="single_domain",
            read_partition_from_df=True,
            df_partitions=df_partitions)
        if not config.full_predictions:
            # remove "train_raw" as it was not predicted
            del dict_df_single_domain["train_raw"]


    # if using the post-processed predictions
    if config.use_post_processed_predictions:
        # insert "post_processed"" after the version number
        file_input_predictions = add_version_suffix_to_file_name(
            file_input_predictions,
            config.version_output, "_post_processed")
    if config.dataset_meta == "SEM_EVAL":
        gpt_label_evaluator = SemEvalGPTLabelEvaluator(
            file_input_predictions=file_input_predictions,
            file_input_ground_truth=par.DICT_FILE_DATA_SEM_EVAL_PROCESSED[config.dataset],
            topic=config.topic, dataset=config.dataset,
            model_gpt=config.model_gpt,
            num_examples_in_prompt=gpt_data_processor._get_num_examples_in_prompt(),
            key_join="ID",
            list_tweet_id_in_prompt=gpt_data_processor._get_list_tweet_id_in_prompt(),
            full_predictions=config.full_predictions,
            use_post_processed_predictions=config.use_post_processed_predictions)
    if "gda" in config.list_eval_modes:
        if config.dataset_meta == "COVID_VACCINE":
            file_output_metrics = par.DICT_RESULT_COVID_VACCINE_MODEL_GPT_METRIC_GDA[config.dataset]
            file_output_confusion_mat = par.DICT_RESULT_COVID_VACCINE_MODEL_GPT_CONFUSION_MATRIX_GDA[config.dataset]
        # if using the post-processed predictions
        if config.use_post_processed_predictions:
            # insert "post_processed"" after the version number
            file_output_metrics = add_version_suffix_to_file_name(
                file_output_metrics,
                config.version_output, "_post_processed")
            file_output_confusion_mat = add_version_suffix_to_file_name(
                file_output_confusion_mat,
                config.version_output, "_post_processed")
        # - evaluate on GDA
        gpt_label_evaluator.evaluate(
            file_output_metrics=file_output_metrics,
            file_output_confusion_mat=file_output_confusion_mat,
            dict_df_eval=dict_df_gda,
            col_name_set="time_domain")
    # - evaluate on single domain
    elif "single_domain" in config.list_eval_modes:
        if config.dataset == "SEM_EVAL":
            file_output_metrics = par.DICT_RESULT_SEM_EVAL_MODEL_GPT_METRIC_SINGLE_DOMAIN[config.dataset]
            file_output_confusion_mat = par.DICT_RESULT_SEM_EVAL_MODEL_GPT_CONFUSION_MATRIX_SINGLE_DOMAIN[config.dataset]
        if config.use_post_processed_predictions:
            # insert "post_processed"" after the version number
            file_output_metrics = file_output_metrics.replace(
                config.version_output, config.version_output + "_post_processed")
            file_output_confusion_mat = file_output_confusion_mat.replace(
                config.version_output, config.version_output + "_post_processed")
        gpt_label_evaluator.evaluate(
            file_output_metrics=file_output_metrics,
            file_output_confusion_mat=file_output_confusion_mat,
            dict_df_eval=dict_df_single_domain,
            col_name_set="set")

    # ----------
    # save the config of this run to a yaml file
    # ----------
    # - save to the output directory
    path_output_eval = get_dir_of_a_file(file_output_metrics)
    creat_dir_for_a_file_if_not_exists(file_output_metrics)
    save_dict_as_yaml(join(path_output_eval, "config_{}_{}_{}_eval.yaml".format(config.version_output, "_".join(config.list_eval_modes), config.topic)), config)


class LabelEvaluator():
    def __init__(self, file_input_predictions, file_input_ground_truth, model_gpt) -> None:
        """_summary_

        Args:
            file_input_predictions (str): the path of the predictions (e.g., labels generated by GPT)
            file_input_ground_truth (str): the file of the ground truth labels
            model_gpt (str): the name of the model used for generating the labels
        """
        assert model_gpt in ["gpt_3_davinci", "flan-t5-xxl", "flan-t5-large", "flan-t5-small", "flan_ul2", "chatgpt_turbo_3_5"]
        self.file_input_predictions = file_input_predictions
        self.file_input_ground_truth = file_input_ground_truth
        self.model_gpt = model_gpt

    def evaluate(self, file_output_metrics, file_output_confusion_mat):
        """Get the metrics of the labels generated by GPT.

        Args:
            file_output_metrics (str): the file of the output metrics
            file_output_confusion_mat (str): the file of the output confusion matrix
        """
        raise NotImplementedError()


class SemEvalGPTLabelEvaluator(LabelEvaluator):
    """Evaluate the stance labels on SemEval dataset generated by GPT.
    """

    def __init__(self, file_input_predictions, file_input_ground_truth, topic, dataset, model_gpt,
                 num_examples_in_prompt, key_join="prompt", list_tweet_id_in_prompt=None, full_predictions=True, use_post_processed_predictions=False) -> None:
        """_summary_

        Args:
            file_input_predictions (str): the file of the labels generated by GPT
            file_input_ground_truth (str): the file of the ground truth labels
            topic (str): the topic of interest, e.g., "Abortion".
            dataset (str): the dataset name, e.g., "SEM_EVAL"
            model_gpt (str): the name of the model used for generating the labels
            num_examples_in_prompt (int): the number of examples in the prompt. Used to check if the removed tweets are indeed the tweets in the prompt.
            key_join (str): the key used for joining the ground truth and the predictions. Default: "prompt"
            list_tweet_id_in_prompt (list): the list of tweet ids in the prompt. Required if key_join="tweet_id". Default: None
            full_predictions (bool): whether the entire set of tweets are predicted. If False, only some of the tweets are predicted (e.g., in the case of using OpenAI's API while saving money). Default: True
            use_post_processed_predictions (bool): whether to use the post-processed predictions (e.g., v9 in single_domain, flan-t5-xxl). Default: False
        """

        super().__init__(file_input_predictions, file_input_ground_truth, model_gpt)
        assert dataset in ["SEM_EVAL"]
        assert key_join in ["prompt", "ID"]
        self.par = get_parameters_for_dataset("SEM_EVAL")
        self.dataset = dataset
        assert topic in self.par.DICT_TOPICS_RAW_TO_NEW[self.dataset].values(
        ) or topic == "all", "The topic should be one of the topics in the dataset or 'all'."

        if key_join == self.par.TEXT_ID:
            assert list_tweet_id_in_prompt is not None
        self.topic = topic
        self.num_examples_in_prompt = num_examples_in_prompt
        self.key_join = key_join
        self.list_tweet_id_in_prompt = list_tweet_id_in_prompt
        self.full_predictions = full_predictions
        self.use_post_processed_predictions = use_post_processed_predictions
        self.col_name_label_true = "label"

        # --------------------
        # read the predictions generated by GPT
        # --------------------
        if self.key_join == "prompt":
            self.df_predictions = pd.read_csv(self.file_input_predictions)
        elif self.key_join == self.par.TEXT_ID:
            self.df_predictions = pd.read_csv(self.file_input_predictions, dtype={self.par.TEXT_ID: str})
        # remove the first column
        if "Unnamed: 0" in self.df_predictions.columns:
            self.df_predictions = self.df_predictions.drop(columns=["Unnamed: 0"])
        # read the ground-truth labels
        self.df_labels = pd.read_csv(self.file_input_ground_truth, dtype={self.par.TEXT_ID: str})

        # --------------------
        # specify the columns of interest
        # --------------------
        if self.model_gpt == "gpt_3_davinci":
            if self.key_join == "prompt":
                self.col_name_key_ground_truth = "tweet_embedded"
                self.col_name_key_predictions = "prompt"
            elif self.key_join == self.par.TEXT_ID:
                self.col_name_key_ground_truth = self.par.TEXT_ID
                self.col_name_key_predictions = self.par.TEXT_ID
            self.col_name_output_label = "Stance"

        elif self.model_gpt in ["flan-t5-xxl", "flan-t5-large", "flan-t5-small", "flan_ul2", "chatgpt_turbo_3_5"]:
            if self.key_join == "prompt":
                self.col_name_key_ground_truth = "tweet_embedded"
                self.col_name_key_predictions = "tweet_embedded"
            elif self.key_join == self.par.TEXT_ID:
                self.col_name_key_ground_truth = self.par.TEXT_ID
                self.col_name_key_predictions = self.par.TEXT_ID
            self.col_name_output_label = "stance_predicted"
        else:
            raise NotImplementedError()

        # --------------------
        # convert the labels and predictions to number
        # --------------------
        if not self.use_post_processed_predictions:
            dict_stance_code = self.par.DICT_STANCES_CODE_GPT[self.model_gpt][self.dataset]
        else:
            dict_stance_code = self.par.DICT_STANCES_CODE[self.dataset]
        # remove the period at the end reponse
        # - to facilitate regex
        self.df_predictions[self.col_name_output_label] = self.df_predictions[self.col_name_output_label].apply(
            lambda x: x[:-1] if x.endswith(".") else x)
        # to lower case
        # - to facilitate regex
        self.df_predictions[self.col_name_output_label] = self.df_predictions[self.col_name_output_label].apply(
            lambda x: x.lower())

        # check if df_predictions[col_name_output_label] is int (int8, 16, 32,...)
        # - use regex to extract the match
        str_regex_match = "|".join([k for k in dict_stance_code.keys()])
        # str_regex_match = "|".join(["^" + k + "$" for k in dict_stance_code.keys()])
        list_stance_found = self.df_predictions[self.col_name_output_label].apply(
            lambda x: re.findall(str_regex_match, x))
        # if there are multiple matches, check if they are the same
        # - if they are the same, use the last match
        # - if they are different, use the last match and record the tweet id
        # -- should save them to a file later (for manual double-checking)
        list_tweet_id_multiple_different_matches = []
        list_tweet_text_multiple_different_matches = []
        for i, list_stance in enumerate(list_stance_found):
            if len(list_stance) > 1:
                if len(set(list_stance)) == 1:
                    list_stance_found[i] = [list_stance[-1]]
                else:
                    list_tweet_id_multiple_different_matches.append(self.df_predictions[self.col_name_key_predictions].iloc[i])
                    list_tweet_text_multiple_different_matches.append(self.df_predictions[self.col_name_output_label].iloc[i])
                    list_stance_found[i] = [list_stance[-1]]

        # - check if the number of matches is 1
        assert all([len(x) == 1 for x in list_stance_found])
        self.df_predictions[self.col_name_output_label] = [x[0] for x in list_stance_found]
        assert all(self.df_labels[self.col_name_label_true].isin(self.par.DICT_STANCES_CODE[self.dataset].keys()))
        assert all(self.df_predictions[self.col_name_output_label].isin(dict_stance_code.keys()))
        self.df_predictions[self.col_name_output_label] = self.df_predictions[self.col_name_output_label].apply(
            lambda x: dict_stance_code[x])
        self.df_labels[self.col_name_label_true] = self.df_labels[self.col_name_label_true].apply(
            lambda x: self.par.DICT_STANCES_CODE[self.dataset][x])
        # the tweet_id of the tweets that have multiple different matches
        self.list_tweet_id_multiple_different_matches = list_tweet_id_multiple_different_matches
        self.list_tweet_text_multiple_different_matches = list_tweet_text_multiple_different_matches

    def evaluate(self, file_output_metrics, file_output_confusion_mat, dict_df_eval=None, col_name_set="set"):
        """Get the metrics of the labels generated by GPT.

        Args:
            file_output_metrics (str): the file of the output metrics
            file_output_confusion_mat (str): the file of the output confusion matrix
            dict_df_eval (dict): the dictionary of the df that I want to evaluate the predictions on, e.g., {train: df_train, val: df_val, test: df_test}. When not provided, the predictions will be evaluated on the whole dataset and also each month.
            col_name_set (str): the column name of the set. Default: "set"
        """
        if dict_df_eval is not None:
            assert isinstance(dict_df_eval, dict)
            assert self.key_join == self.par.TEXT_ID

        df_predictions = self.df_predictions
        df_labels = self.df_labels

        # --------------------
        # set the parameters when `use_post_processed_predictions = True`
        # --------------------
        if self.use_post_processed_predictions:
            file_output_metrics = file_output_metrics.replace(".csv", "_post_processed.csv")
            file_output_confusion_mat = file_output_confusion_mat.replace(".csv", "_post_processed.csv")
        # --------------------
        # specify the columns of interest
        # --------------------
        if self.model_gpt == "gpt_3_davinci":
            if self.key_join == "prompt":
                col_name_key_ground_truth = "tweet_embedded"
                col_name_key_predictions = "prompt"
            elif self.key_join == self.par.TEXT_ID:
                col_name_key_ground_truth = self.par.TEXT_ID
                col_name_key_predictions = self.par.TEXT_ID
            col_name_output_label = "Stance"

        elif self.model_gpt in ["flan-t5-xxl","flan-t5-large","flan-t5-small", "flan_ul2", "chatgpt_turbo_3_5"]:
            if self.key_join == "prompt":
                col_name_key_ground_truth = "tweet_embedded"
                col_name_key_predictions = "tweet_embedded"
            elif self.key_join == self.par.TEXT_ID:
                col_name_key_ground_truth = self.par.TEXT_ID
                col_name_key_predictions = self.par.TEXT_ID
            col_name_output_label = "stance_predicted"
        else:
            raise ValueError(f"self.model_gpt = {self.model_gpt}")

        if self.key_join == "prompt":
            # --------------------
            # sanity check: check if duplicated tweets have the same labels and predictions
            # --------------------
            df_labels_duplicated = df_labels[df_labels.duplicated(
                subset=[col_name_key_ground_truth], keep=False)]
            df_predictions_duplicated = df_predictions[df_predictions.duplicated(
                subset=[col_name_key_predictions], keep=False)]

            assert all(df_labels_duplicated.groupby(["tweet_embedded"]).agg(
                label_unique=(self.col_name_label_true, lambda x: x.nunique())
            ).reset_index()["label_unique"] == 1)

            assert all(df_predictions_duplicated.groupby([col_name_key_predictions]).agg(
                label_unique=(col_name_output_label, lambda x: x.nunique())
            ).reset_index()["label_unique"] == 1)
            # --------------------
            # drop duplicated tweets
            # --------------------
            # - keep the first one and ignore the other duplicated ones
            df_predictions = df_predictions.drop_duplicates(subset=[col_name_key_predictions], keep='first')

            df_labels = df_labels.drop_duplicates(subset=[col_name_key_ground_truth], keep='first')

        # remove the first column
        if "Unnamed: 0" in df_labels.columns:
            df_labels = df_labels.drop(columns=["Unnamed: 0"])
        # --------------------
        # merge the labels and the processed data
        # --------------------
        df_merged = pd.merge(df_predictions, df_labels,
                             left_on=col_name_key_predictions,
                             right_on=col_name_key_ground_truth)
        if self.full_predictions:
            assert len(df_merged) == len(df_predictions) == len(df_labels)
        else:
            assert len(df_merged) == len(df_predictions) <= len(df_labels)

        # --------------------
        # remove the tweets that are in the prompt
        # - these tweets have been "seen" already
        # --------------------
        # check if the prompt is in the tweet
        # - true is the count is 2
        if self.key_join == "prompt":
            df_merged["is_prompt_in_tweet"] = df_merged.apply(
                lambda x: x["tweet_embedded"].count(x["tweet"]) == 2, axis=1)
        elif self.key_join == self.par.TEXT_ID:
            df_merged["is_prompt_in_tweet"] = df_merged.apply(
                lambda x: x[self.par.TEXT_ID] in self.list_tweet_id_in_prompt, axis=1)
        if self.full_predictions:
            assert df_merged["is_prompt_in_tweet"].sum() == self.num_examples_in_prompt * self.par.DICT_NUM_CLASS[self.dataset]
        else:
            assert df_merged["is_prompt_in_tweet"].sum() <= self.num_examples_in_prompt * self.par.DICT_NUM_CLASS[self.dataset]
        # - remove the tweets that are in the prompt
        df_merged = df_merged[~df_merged["is_prompt_in_tweet"]]

        # # --------------------
        # # convert the labels and predictions to number
        # # --------------------
        # # check if df_predictions[col_name_output_label] is int (int8, 16, 32,...)
        # if df_merged[col_name_output_label].dtype.kind not in 'iufc':
        #     # - use regex to extract the match
        #     str_regex_match = "|".join(self.par.DICT_STANCES_CODE_GPT[self.model_gpt][self.dataset].keys())
        #     list_stance_found = df_merged[col_name_output_label].apply(
        #         lambda x: re.findall(str_regex_match, x))
        #     # - check if the number of matches is 1
        #     assert all([len(x) == 1 for x in list_stance_found])
        #     df_merged[col_name_output_label] = [x[0] for x in list_stance_found]
        #     assert all(df_merged[self.col_name_label_true].isin(self.par.DICT_STANCES_CODE[self.dataset].keys()))
        #     assert all(df_merged[col_name_output_label].isin(self.par.DICT_STANCES_CODE_GPT[self.model_gpt][self.dataset].keys()))
        #     df_merged[self.col_name_label_true] = df_merged[self.col_name_label_true].apply(
        #         lambda x: self.par.DICT_STANCES_CODE[self.dataset][x])
        #     df_merged[col_name_output_label] = df_merged[col_name_output_label].apply(
        #         lambda x: self.par.DICT_STANCES_CODE_GPT[self.model_gpt][self.dataset][x])
        #     df_predictions[col_name_output_label] = df_predictions[col_name_output_label].apply(
        #         lambda x: self.par.DICT_STANCES_CODE_GPT[self.model_gpt][self.dataset][x])
        #     df_labels[self.col_name_label_true] = df_labels[self.col_name_label_true].apply(
        #         lambda x: self.par.DICT_STANCES_CODE[self.dataset][x])

        # --------------------
        # Evaluate by all the partitions
        # --------------------
        # - the tweets that are in all the partitions
        df_partitions_all = pd.DataFrame(columns=[self.par.TEXT_ID])
        for partition in dict_df_eval.keys():
            df_partition = dict_df_eval[partition]
            df_partitions_all = pd.concat([df_partitions_all, df_partition[[self.par.TEXT_ID]]])
        # - filter the tweets that are in all the partitions
        df_merged_all = df_merged[df_merged[self.par.TEXT_ID].isin(df_partitions_all[self.par.TEXT_ID])]
        metrics = self.compute_metrics_without_trainer(df_merged_all[self.col_name_label_true].to_numpy(),
                                                       df_merged_all[col_name_output_label].to_numpy(),
                                                       ["f1", "accuracy", "recall", "precision"],
                                                       self.dataset,
                                                       by_class=True,
                                                       num_classes=self.par.DICT_NUM_CLASS[self.dataset])

        metrics[col_name_set] = "all"

        # dict_df_eval

        # --------------------
        # Evaluate by each partition or month
        # --------------------
        # -- create a dataframe to store the metrics
        df_metrics = pd.DataFrame(columns=metrics.keys())
        # -- insert metrics dict to the dataframe
        df_metrics = pd.concat([df_metrics, pd.DataFrame([metrics])])
        if dict_df_eval is None:
            # - evaluate by each time domain
            for time_domain in self.par.DICT_TIMES[self.par.TIME_DOMAIN_UNIT][self.dataset]:
                name_time_domain = convert_time_unit_into_name(time_domain)
                # make sure that the time domain is in the dataframe
                assert name_time_domain in df_merged["time_unit"].unique()
                metrics_time_domain = self.compute_metrics_without_trainer(
                    df_merged[df_merged["time_unit"] == name_time_domain][self.col_name_label_true].to_numpy(),
                    df_merged[df_merged["time_unit"] == name_time_domain][col_name_output_label].to_numpy(),
                    ["f1", "accuracy", "recall", "precision"],
                    self.dataset,
                    logits=None,
                    by_class=True,
                    num_classes=self.par.DICT_NUM_CLASS[self.dataset])
                metrics_time_domain[col_name_set] = name_time_domain
                df_metrics = pd.concat([df_metrics, pd.DataFrame([metrics_time_domain])])
            # - reorder the columns such that col_name_set column is the first column
            df_metrics = df_metrics[[col_name_set] + [col for col in df_metrics.columns if col != col_name_set]]
        else:
            # - evaluate by each partition
            # - e.g. {train: df_train, dev: df_dev, test: df_test}
            for partition in dict_df_eval.keys():
                df_partition = dict_df_eval[partition]
                # join the partition with the predictions using tweet_id as the key
                df_merged_partition = pd.merge(df_partition, df_predictions,
                                               left_on=col_name_key_ground_truth,
                                               right_on=col_name_key_predictions)

                if len(df_merged_partition) == 0:
                    assert self.full_predictions is False
                    continue
                if self.full_predictions:
                    assert 0 < len(df_merged_partition) == len(df_partition)
                else:
                    assert 0 < len(df_merged_partition) <= len(df_predictions)
                metrics_partition = self.compute_metrics_without_trainer(
                    df_merged_partition["label"].to_numpy(),
                    df_merged_partition[col_name_output_label].to_numpy(),
                    ["f1", "accuracy", "recall", "precision"],
                    self.dataset,
                    logits=None,
                    by_class=True,
                    num_classes=self.par.DICT_NUM_CLASS[self.dataset])
                metrics_partition[col_name_set] = partition
                df_metrics = pd.concat([df_metrics, pd.DataFrame([metrics_partition])])

        # save the metrics
        creat_dir_for_a_file_if_not_exists(file_output_metrics)
        df_metrics.to_csv(file_output_metrics)

        # --------------------
        # get the confusion matrix
        # --------------------
        # - the entire dataset
        df_confusion_mat = pd.crosstab(df_merged[self.col_name_label_true], df_merged[col_name_output_label],
                                       rownames=['True'], colnames=['Pred'])
        df_confusion_mat = df_confusion_mat.reset_index().rename_axis(None, axis=1)

        # -- rename the columns
        df_confusion_mat.columns = ["true_class"] + ["pred_" + class_name for class_name in self.par.DICT_STANCES_CODE[self.dataset].keys()]
        df_confusion_mat["data_set"] = "all"

        # - evaluate by each time domain
        if dict_df_eval is None:
            for time_domain in self.par.DICT_TIMES[self.par.TIME_DOMAIN_UNIT][self.dataset]:
                name_time_domain = convert_time_unit_into_name(time_domain)
                df_confusion_mat_time_domain = pd.crosstab(df_merged[df_merged["time_unit"] == name_time_domain][self.col_name_label_true],
                                                           df_merged[df_merged["time_unit"] == name_time_domain]
                                                           [col_name_output_label], rownames=['True'], colnames=['Pred'])
                df_confusion_mat_time_domain = df_confusion_mat_time_domain.reset_index().rename_axis(None, axis=1)
                df_confusion_mat_time_domain.columns = ["true_class"] + ["pred_" + class_name for class_name in self.par.DICT_STANCES_CODE[self.dataset].keys()]
                df_confusion_mat_time_domain["data_set"] = name_time_domain
                df_confusion_mat = pd.concat([df_confusion_mat, df_confusion_mat_time_domain])
                # -- rename the values in true_class column
                df_confusion_mat["true_class"] = df_confusion_mat["true_class"].apply(
                    lambda x: "true_" + list(self.par.DICT_STANCES_CODE[self.dataset].keys())[x])
        else:
            # - evaluate by each partition
            # - e.g. {train: df_train, dev: df_dev, test: df_test}
            list_df_confusion = []
            for partition in dict_df_eval.keys():
                df_partition = dict_df_eval[partition]
                # join the partition with the predictions using tweet_id as the key
                df_merged_partition = pd.merge(df_partition, df_predictions,
                                               left_on=col_name_key_ground_truth,
                                               right_on=col_name_key_predictions)
                if len(df_merged_partition) == 0:
                    assert self.full_predictions is False
                    continue
                if self.full_predictions:
                    assert len(df_merged_partition) == len(df_partition)
                else:
                    assert 0 < len(df_merged_partition) <= len(df_predictions)
                confusion_matrix_this_set = confusion_matrix(df_merged_partition["label"].to_numpy(),
                                                             df_merged_partition[col_name_output_label].to_numpy(),
                                                             labels=list(self.par.DICT_STANCES_CODE[self.dataset].values()))
                df_confusion_matrix_this_set = pd.DataFrame(confusion_matrix_this_set)
                df_confusion_matrix_this_set.columns = ["pred_" + key for key in self.par.DICT_STANCES_CODE[self.dataset].keys()]
                # prepend a column to indicate the true label
                df_confusion_matrix_this_set.insert(0, "true_class", ["true_" + key for key in self.par.DICT_STANCES_CODE[self.dataset].keys()])
                # prepend a column to indicate the partitions
                df_confusion_matrix_this_set.insert(0, "data_set", partition)
                list_df_confusion.append(df_confusion_matrix_this_set)
                df_confusion_mat = pd.concat(list_df_confusion)
        # - reorder the columns such that "data_set" column is the first column
        df_confusion_mat = df_confusion_mat[["data_set"] + [col for col in df_confusion_mat.columns if col != "data_set"]]
        # save the confusion matrix
        creat_dir_for_a_file_if_not_exists(file_output_confusion_mat)
        # save csv without index
        df_confusion_mat.to_csv(file_output_confusion_mat, index=False)

        # --------------------
        # save the list_tweet_id_multiple_different_matches as a csv file
        # --------------------
        if len(self.list_tweet_id_multiple_different_matches) > 0:
            # columns = [self.par.TEXT_ID, "text"]
            # list_tweet_id_multiple_different_matches
            # list_tweet_text_multiple_different_matches
            df_multiple_different_matches = pd.DataFrame(
                {
                    self.par.TEXT_ID: self.list_tweet_id_multiple_different_matches,
                    self.col_name_output_label + "_raw": self.list_tweet_text_multiple_different_matches
                }
            )
            # get the output dir of 'file_output_confusion_mat'
            path_output = dirname(file_output_confusion_mat)
            file_output_multiple_matches = join(path_output, "tweets_multiple_different_matches.csv")
            df_multiple_different_matches.to_csv(file_output_multiple_matches, index=False)

    def compute_metrics_without_trainer(self, labels, predictions, list_metric, dataset, logits=None, by_class=False, num_classes=None):
        """Compute the metrics of the labels generated by GPT.

        Args:
            labels (np.ndarray): the ground-truth labels
            predictions (np.ndarray): the predicted labels
            list_metric (str): e.g., ["f1,accuracy,recall,precision","roc_auc"]
            dataset (str): the dataset name, e.g., "COVID_VACCINE_Q1"
            logits (np.ndarray): the logits of the predictions. Only needed for the ROC-AUC score. Defaults to None.
            by_class (bool, optional): Whether to compute metrics for each class (even if there are only two classes). Defaults to False.
            num_classes (int, optional): The number of classes. Only used when `by_class=True`. Defaults to None.
        Returns:
            metrics (dict): the metrics
        """
        # assert dataset in ["COVID_VACCINE_Q1", "COVID_VACCINE_Q2"]
        # assert question in ["Q1", "Q2"]

        return compute_metrics_without_trainer(labels, predictions, list_metric, dataset, logits, by_class, num_classes)


if __name__ == "__main__":
    pass