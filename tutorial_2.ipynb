{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Stance Detection on Tweets using NLP Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Objective of the tutorial**: This tutorial will guide you through the process of stance detection on tweets using two main approaches: fine-tuning a BERT model and using large language models (LLMs).\n",
    "\n",
    "**Prerequisites**: Basic Python skills and ML knowledge. Familiarity with NLP concepts is a plus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Stance Detection and Why is it Important?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stance detection is an essential task in natural language processing that aims to determine the attitude or position expressed by an author towards a specific target, such as an entity, topic, or claim. The output of stance detection is typically a categorical label, such as \"in-favor,\" \"against,\" or \"neutral,\" indicating the stance of the author in relation to the target. This task is critical for studying human belief dynamics, e.g., how people influence each other's opinions and how beliefs change over time.\n",
    "\n",
    "There are two key challenges in stance detection, especially when working with large datasets like Twitter data. First, the underlying attitude expressed in the text is often subtle, which requires domain knowledge and context to correctly label the stance. Second, the corpus can be very large, with millions of tweets, making it impractical to manually annotate all of them.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO]: Provide a few example tweets of stance detection in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we will focus on stance detection in the context of COVID-19 vaccination tweets. We will analyze a dataset containing tweets about COVID-19 vaccination, with each tweet labeled as either in-favor, against, or neutral with respect to COVID-19 vaccination. Our goal is to develop a model that can accurately identify the stance expressed in these tweets.\n",
    "\n",
    "To address these challenges, we will leverage advanced natural language processing (NLP) techniques like BERT and large language models (LLMs). BERT and LLMs are pre-trained on massive corpora, enabling them to capture subtle contextual information and better understand the nuances of language. With these NLP models, we can effectively adapt their general language understanding to the specific task of stance detection, even in cases where domain knowledge is required. This approach allows us to process large amounts of data with high accuracy while significantly reducing the need for manual annotation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding BERT\n",
    "\n",
    "In this section, we will briefly introduce BERT, a powerful NLP model that has been widely used in many NLP tasks. We will also discuss how BERT can be fine-tuned for stance detection.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is BERT and how it works\n",
    "\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking natural language processing (NLP) model that has taken the world by storm. Created by researchers at Google in 2018, BERT has revolutionized the way we understand and analyze language. The model is designed to learn useful representations for words from unlabeled text, which can then be fine-tuned for a wide range of NLP tasks, such as stance detection, sentiment analysis, question-answering, among many.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, BERT is a powerful NLP model that leverages bidirectional context, Transformer architecture, and a pre-training and fine-tuning approach to achieve state-of-the-art performance on a wide range of tasks. I will describe each of these components in more detail below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a link to the BERT iteactive tutorial here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bidirectional Context: Understanding Context in Both Directions\n",
    "\n",
    "Language is complex, and understanding it is no simple task. Traditional NLP models have focused on reading text in one direction, either from left-to-right or right-to-left, making it difficult for them to grasp the full context of a word or phrase [TODO: should check whether this is true and should add examples]. BERT, however, is designed to process text in both directions, allowing it to understand the meaning of words based on the words that come before and after them. This bidirectional approach helps BERT capture the subtle nuances of language and produce more accurate results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a figure to illustrate the bidirectional context here.\n",
    "[TODO] Should add a example to show why bi-directional context is important here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### A Powerful Architecture: Transformers\n",
    "\n",
    "BERT is built upon the Transformer architecture, which was introduced by Vaswani et al. in 2017. Transformers are a type of neural network architecture that rely on self-attention mechanisms to process input sequences in parallel, rather than sequentially as in traditional recurrent neural networks (RNNs) or long short-term memory (LSTM) networks. This parallel processing allows Transformers to be highly efficient in handling long sequences of text, making them well-suited for NLP tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a figure to show the self-attention mechanism here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Pre-training and Fine-tuning: Learning from Lots of Text and Adapting to Specific Tasks\n",
    "\n",
    "One of the key secrets behind BERT's success is its ability to learn from vast amounts of text and then adapt that knowledge to specific tasks. It is usually composed of two stages: pre-training and fine-tuning.\n",
    "\n",
    "##### Pre-training phase\n",
    "\n",
    "During the initial pre-training phase, BERT is exposed to massive amounts of text from sources like Wikipedia and online books, allowing it to learn general language understanding. During the pre-training phase, BERT learns to predict missing words in a sentence (masked language modeling) and to determine if two sentences follow each other (next sentence prediction). This phase allows BERT to learn the relationships between words even without any task-specific labels (e.g., stance labels are not needed for pre-training).\n",
    "\n",
    "##### Fine-tuning phase\n",
    "\n",
    "After pre-training, BERT can be fine-tuned for a specific task with a smaller labeled dataset (e.g., tweets with stance labels). Fine-tuning involves updating the model's weights using the labeled data, allowing BERT to adapt its general language understanding to the specific task. This process is relatively fast and requires less training data compared to training a model from scratch.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### BERT sub-word tokenization\n",
    "\n",
    "One caveat of BERT is that it requires a special \"subword-tokenization\" process (i.e., WordPiece tokenization). That is, it deos not directly encode each individual word, but rather encode each word as a sequence of \"sub-word tokens\". For example, the word \"university\" can be broken down into the subwords \"uni\" and \"versity,\" which are more likely to appear in the corpus than the word \"university\" itself. This process of breaking down words into subwords is called sub-word tokenization.\n",
    "\n",
    "Sub-word tokenization is important for several reasons. Just to name two important ones:\n",
    "\n",
    "#### Consistent Representation of Similar Words\n",
    "\n",
    "Tokenization ensures that the text is represented in a consistent manner, making it easier for the model to learn and identify patterns in the data. By breaking the text into tokens, the model can focus on the essential units of meaning, allowing it to better understand and analyze the input.\n",
    "For an example, let us consider the following two words: \"anti-vaccine\" and \"antitrust\".\n",
    "\n",
    "These words share a common prefix \"anti-\", but they are related to different topics. Tokenization can help standardize the text by breaking them down into smaller, overlapping tokens, i.e., [\"anti\", \"-\", \"vaccine\"] and [\"anti\", \"trust\"]. \n",
    "\n",
    "By representing the words as a sequence of tokens, the model can more effectively identify the commonality between them (the shared \"anti\" prefix) while also distinguishing the unique parts (\"-vaccine\" and \"trust\"). This approach helps the model learn the relationships between word parts and the overall meaning of words in a more generalizable way, while also capturing the nuances that make each word unique.\n",
    "\n",
    "#### Handling Out-of-Vocabulary Words \n",
    "\n",
    "One of the challenges in NLP is dealing with words that the model has not encountered during training, also known as out-of-vocabulary (OOV) words. By using tokenization, BERT can handle OOV words more effectively. Subword tokenization breaks down words into smaller, meaningful parts that the model has likely seen before, allowing it to better understand and process previously unseen words.\n",
    "\n",
    "For example, suppose we have a sentence containing a relatively newly-coined word: \"Anti-vaxxer\".\n",
    "\n",
    "Here, the word \"anti-vaxxer\" is a neologism that may not be present in the model's vocabulary, particularly if the model was trained on older data. If we used a simple word-based tokenization, the model would struggle to process this word. However, using a subword tokenization approach, the word can be broken down into smaller parts that the model has likely seen before:\n",
    "\n",
    "[\"anti\", \"-\", \"vaxx\", \"er\"]\n",
    "\n",
    "This breakdown allows the model to infer the meaning of the previously unseen word based on the subword components it has encountered during training. The model can recognize the \"anti\" prefix and the similarity of \"vaxx\" to \"vacc\" (as in \"vaccine\"). This enables BERT to better understand and process out-of-vocabulary words, especially those that are relatively new or coined, making it more robust and adaptable to a wide range of text inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-tuning a BERT Model with HuggingFace\n",
    "\n",
    "Now, let's fine-tune a BERT model using the HuggingFace Transformers library.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installing HuggingFace Transformers library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and processing the labeled dataset\n",
    "_Instructions on loading and processing the labeled dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model for stance detection\n",
    "_Guide on how to fine-tune the BERT model for stance detection_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model and analyzing results\n",
    "_Methods for evaluating the model and analyzing the results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and analyze the results here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Large Language Models (LLM) for Stance Detection\n",
    "\n",
    "Large Language Models (LLMs) are a type of advanced natural language processing model that has gained significant attention in recent years. These models are designed to understand and generate human-like text by learning from vast amounts of data. In the context of stance detection, LLMs can offer powerful and flexible tools to analyze and classify text based on the stance or attitude expressed towards a particular topic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training: Learning from Massive Amounts of Text\n",
    "\n",
    "Like BERT model, LLMs also rely on pre-training to learn from massive amounts of text. During pre-training, LLMs are exposed to a large corpus of text, which allows them to learn the structure and style of human language. By learning from a diverse range of text sources, LLMs can build a rich understanding of language, including grammar, vocabulary, and context. This extensive knowledge can be particularly useful for detecting stances in text.\n",
    "\n",
    "\n",
    "During training, LLMs optimize their parameters to minimize a loss function, which is a measure of the difference between the model's predictions and the actual target outputs. In the case of language models, the loss function is typically based on the likelihood of the correct next word (or token) given the context. By minimizing this loss function, the model learns to generate text that closely resembles the structure and style of the training data.\n",
    "\n",
    "[TODO: add a diagram to show the loss function]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast with BERT\n",
    " \n",
    "\n",
    "#### Model Size and Traing Corpus Size\n",
    "\n",
    "While BERT is considered a Large Language Model, it has a much smaller number of parameters in the model, and has a smaller training data during the pre-training phase, compared to some of the more recent LLMs. Because of this constraint, BERT typically requires fine-tuning on a specific task, using a labeled dataset, to perform optimally.\n",
    "\n",
    "Newer LLMs, like GPT-3 or Flan-T5, have been trained on even larger datasets and have demonstrated remarkable capabilities, including the ability to perform tasks with little or no fine-tuning. This is due to their extensive training, which allows them to generate more accurate and coherent responses in a variety of situations, including stance detection.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: add a table to illustrate the difference between BERT and LLMs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The Potential of Prompting\n",
    "\n",
    "While BERT is designed to be fine-tuned on specific tasks like stance detection using labeled data, some of these more recent LLMs can perform stance detection without fine-tuning, but with prompting techniques (i.e., the way you \"ask\" these models questions). These techniques involve providing the model with context or examples to guide its response, rather than relying on fine-tuning with labeled data.\n",
    "\n",
    "For example, with a zero-shot approach, an LLM can perform stance detection on tweets without being fine-tuned on a specific dataset. The model can understand the task and generate an appropriate response based on its extensive knowledge learned during pretraining."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: add a table to illustrate the difference between BERT and LLMs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Open-source model flan-t5\n",
    "\n",
    "_Brief introduction to flan-t5_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### OpenAI's GPT 3.5\n",
    "\n",
    "_Brief introduction to GPT 3.5_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompting Techniques for LLMs\n",
    "\n",
    "Let's explore three different prompting techniques for large language models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Zero-shot prompting\n",
    "\n",
    "_Description of zero-shot prompting_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Few-shot prompting\n",
    "\n",
    "_Description of few-shot prompting_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chain-of-thoughts method\n",
    "\n",
    "_Description of the chain-of-thoughts method_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Stance Detection using LLMs\n",
    "\n",
    "Now, let's implement stance detection using flan-t5 and GPT 3.5.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setting up the environment\n",
    "\n",
    "_Instructions for setting up the environment, including installing required libraries_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up the environment here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input prompts for each method\n",
    "\n",
    "_Guide on how to create input prompts for each prompting technique_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare input prompts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing stance detection with flan-t5 and GPT 3.5\n",
    "\n",
    "_Instructions for implementing stance detection using the two LLMs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement stance detection using flan-t5 and GPT 3.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing results and performance\n",
    "\n",
    "_Methods for comparing the results and performance of the two LLMs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare results and performance here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to perform stance detection on tweets using a fine-tuned BERT model and large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "435414adc4ce3b84c9fac3213024ef17f383b5213bd6fd3489d95b5052ab39f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
