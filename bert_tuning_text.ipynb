{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Stance Detection on Tweets using NLP Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Objective of the tutorial**: This tutorial will guide you through the process of stance detection on tweets using two main approaches: fine-tuning a BERT model and using large language models (LLMs).\n",
    "\n",
    "**Prerequisites**: Basic Python skills and ML knowledge. Familiarity with NLP concepts is a plus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Stance Detection and Why is it Important?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stance detection is an essential task in natural language processing that aims to determine the attitude or position expressed by an author towards a specific target, such as an entity, topic, or claim. The output of stance detection is typically a categorical label, such as \"in-favor,\" \"against,\" or \"neutral,\" indicating the stance of the author in relation to the target. This task is critical for studying human belief dynamics, e.g., how people influence each other's opinions and how beliefs change over time.\n",
    "\n",
    "There are two key challenges in stance detection, especially when working with large datasets like Twitter data. First, the underlying attitude expressed in the text is often subtle, which requires domain knowledge and context to correctly label the stance. Second, the corpus can be very large, with millions of tweets, making it impractical to manually annotate all of them.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/stance_vs_sentiment.png\" width=\"80%\" height=\"80%\"> -->\n",
    "![stance_vs_sentiment](images/stance_vs_sentiment.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we will focus on stance detection in the context of the \"Abortion\" topic using the SemEval-2016 dataset. We chose the abortion topic because it is currently a hotly debated issue, and it is important to understand public opinion on this matter. We will analyze a dataset containing tweets about abortion, with each tweet labeled as either in-favor, against, or neutral with respect to the topic. Our goal is to develop a model that can accurately identify the stance expressed in these tweets.\n",
    "\n",
    "To address these challenges, we will leverage advanced natural language processing (NLP) techniques like BERT and large language models (LLMs). BERT and LLMs are pre-trained on massive corpora, enabling them to capture subtle contextual information and better understand the nuances of language. With these NLP models, we can effectively adapt their general language understanding to the specific task of stance detection, even in cases where domain knowledge is required. This approach allows us to process large amounts of data with high accuracy while significantly reducing the need for manual annotation.\n",
    "\n",
    "Note: The SemEval-2016 dataset contains tweets related to six different topics: Abortion, Atheism, Climate Change, Feminist Movement, Hillary Clinton, and Legalization of Abortion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](images/dataset_semeval_2016.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Stance Detection Paradigms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![two_paradigm](images/stance_detection_two_paradigm.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add contrast between the two paradigms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Paradigm 1: using BERT for stance detection\n",
    "\n",
    "In this section, we will briefly introduce BERT, a powerful NLP model that has been widely used in many NLP tasks. We will also discuss how BERT can be fine-tuned for stance detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is BERT and how it works\n",
    "\n",
    "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking natural language processing (NLP) model that has taken the world by storm. Created by researchers at Google in 2018, BERT has revolutionized the way we understand and analyze language. The model is designed to learn useful representations for words from unlabeled text, which can then be fine-tuned for a wide range of NLP tasks, such as stance detection, sentiment analysis, question-answering, among many.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, BERT is a powerful NLP model that leverages bidirectional context, Transformer architecture, and a pre-training and fine-tuning approach to achieve state-of-the-art performance on a wide range of tasks. I will describe each of these components in more detail below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a link to the transformer iteactive tutorial here.\n",
    "[http://jalammar.github.io/illustrated-bert/](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bidirectional Context: Understanding Context in Both Directions\n",
    "\n",
    "Language is complex, and understanding it is no simple task. Traditional NLP models have focused on reading text in one direction, either from left-to-right or right-to-left, making it difficult for them to grasp the full context of a word or phrase [TODO: should check whether this is true and should add examples]. BERT, however, is designed to process text in both directions, allowing it to understand the meaning of words based on the words that come before and after them. This bidirectional approach helps BERT capture the subtle nuances of language and produce more accurate results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT](images/bert_architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a figure to illustrate the bidirectional context here.\n",
    "[TODO] Should add a example to show why bi-directional context is important here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### A Powerful Architecture: Transformers\n",
    "\n",
    "BERT is built upon the Transformer architecture, which was introduced by Vaswani et al. in their 2017 paper, \"Attention Is All You Need.\" The Transformer architecture is composed of \"encoder\" and \"decoder\" layers, where the encoder reads the input text, and the decoder generates the output text. The key component of the architecture is the \"self-attention mechanism,\" which helps the model identify important parts of the input text and understand the relationships between words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/bert_base.png](images/bert_base.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO]: add the reference to this tutorial: http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a figure to show the self-attention mechanism here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Pre-training and Fine-tuning: Learning from Lots of Text and Adapting to Specific Tasks\n",
    "\n",
    "One of the key secrets behind BERT's success is its ability to learn from vast amounts of text and then adapt that knowledge to specific tasks. It is usually composed of two stages: pre-training and fine-tuning.\n",
    "\n",
    "##### Pre-training phase\n",
    "\n",
    "During the initial pre-training phase, BERT is exposed to massive amounts of text from sources like Wikipedia and online books, allowing it to learn general language understanding. During the pre-training phase, BERT learns to predict missing words in a sentence (masked language modeling) and to determine if two sentences follow each other (next sentence prediction). This phase allows BERT to learn the relationships between words even without any task-specific labels (e.g., stance labels are not needed for pre-training).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/bert_pretrain.png](images/bert_pretrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Fine-tuning phase\n",
    "\n",
    "After pre-training, BERT can be fine-tuned for a specific task with a smaller labeled dataset (e.g., tweets with stance labels). Fine-tuning involves updating the model's weights using the labeled data, allowing BERT to adapt its general language understanding to the specific task. This process is relatively fast and requires less training data compared to training a model from scratch.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/bert_fine_tune.png](images/bert_fine_tune.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Should add a diagram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### BERT's Sub-word Tokenization\n",
    "\n",
    "One caveat of BERT is that it requires a special \"subword-tokenization\" process (i.e., WordPiece tokenization). That is, it deos not directly encode each individual word, but rather encode each word as a sequence of \"sub-word tokens\". For example, the word \"university\" can be broken down into the subwords \"uni\" and \"versity,\" which are more likely to appear in the corpus than the word \"university\" itself. This process of breaking down words into subwords is called sub-word tokenization.\n",
    "\n",
    "Sub-word tokenization is important for several reasons. Just to name two important ones:\n",
    "\n",
    "#### Consistent Representation of Similar Words\n",
    "\n",
    "Tokenization ensures that the text is represented in a consistent manner, making it easier for the model to learn and identify patterns in the data. By breaking the text into tokens, the model can focus on the essential units of meaning, allowing it to better understand and analyze the input.\n",
    "For an example, let us consider the following two words: \"anti-vaccine\" and \"antitrust\".\n",
    "\n",
    "These words share a common prefix \"anti-\", but they are related to different topics. Tokenization can help standardize the text by breaking them down into smaller, overlapping tokens, i.e., [\"anti\", \"-\", \"vaccine\"] and [\"anti\", \"trust\"]. \n",
    "\n",
    "By representing the words as a sequence of tokens, the model can more effectively identify the commonality between them (the shared \"anti\" prefix) while also distinguishing the unique parts (\"-vaccine\" and \"trust\"). This approach helps the model learn the relationships between word parts and the overall meaning of words in a more generalizable way, while also capturing the nuances that make each word unique.\n",
    "\n",
    "#### Handling Out-of-Vocabulary Words \n",
    "\n",
    "One of the challenges in NLP is dealing with words that the model has not encountered during training, also known as out-of-vocabulary (OOV) words. By using tokenization, BERT can handle OOV words more effectively. Subword tokenization breaks down words into smaller, meaningful parts that the model has likely seen before, allowing it to better understand and process previously unseen words.\n",
    "\n",
    "For example, suppose we have a sentence containing a relatively newly-coined word: \"Anti-vaxxer\".\n",
    "\n",
    "Here, the word \"anti-vaxxer\" is a neologism that may not be present in the model's vocabulary, particularly if the model was trained on older data. If we used a simple word-based tokenization, the model would struggle to process this word. However, using a subword tokenization approach, the word can be broken down into smaller parts that the model has likely seen before:\n",
    "\n",
    "[\"anti\", \"-\", \"vaxx\", \"er\"]\n",
    "\n",
    "This breakdown allows the model to infer the meaning of the previously unseen word based on the subword components it has encountered during training. The model can recognize the \"anti\" prefix and the similarity of \"vaxx\" to \"vacc\" (as in \"vaccine\"). This enables BERT to better understand and process out-of-vocabulary words, especially those that are relatively new or coined, making it more robust and adaptable to a wide range of text inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise: Fine-tuning a BERT Model with HuggingFace\n",
    "\n",
    "Now, let's fine-tune a BERT model using the HuggingFace Transformers library.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Installing HuggingFace Transformers library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and processing the labeled dataset\n",
    "_Instructions on loading and processing the labeled dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model for stance detection\n",
    "_Guide on how to fine-tune the BERT model for stance detection_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model and analyzing results\n",
    "_Methods for evaluating the model and analyzing the results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and analyze the results here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paradigm 2: Using Large Language Models (LLM) for Stance Detection\n",
    "\n",
    "Large Language Models (LLMs) are a type of advanced natural language processing model that has gained significant attention in recent years. These models are designed to understand and generate human-like text by learning from vast amounts of data. In the context of stance detection, LLMs can offer powerful and flexible tools to analyze and classify text based on the stance or attitude expressed towards a particular topic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training: Learning from Massive Amounts of Text\n",
    "\n",
    "Like BERT model, LLMs also rely on pre-training to learn from massive amounts of text. During pre-training, LLMs are exposed to a large corpus of text, which allows them to learn the structure and style of human language. By learning from a diverse range of text sources, LLMs can build a rich understanding of language, including grammar, vocabulary, and context. This extensive knowledge can be particularly useful for detecting stances in text.\n",
    "\n",
    "\n",
    "During training, LLMs optimize their parameters to minimize a loss function, which is a measure of the difference between the model's predictions and the actual target outputs. In the case of language models, the loss function is typically based on the likelihood of the correct next word (or token) given the context. By minimizing this loss function, the model learns to generate text that closely resembles the structure and style of the training data.\n",
    "\n",
    "[TODO: add a diagram to show the loss function]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast with BERT\n",
    " \n",
    "\n",
    "#### Model Size and Traing Corpus Size\n",
    "\n",
    "While BERT is considered a Large Language Model, it has a much smaller number of parameters in the model, and has a smaller training data during the pre-training phase, compared to some of the more recent LLMs. Because of this constraint, BERT typically requires fine-tuning on a specific task, using a labeled dataset, to perform optimally.\n",
    "\n",
    "Newer LLMs, like GPT-3 or Flan-T5, have been trained on even larger datasets and have demonstrated remarkable capabilities, including the ability to perform tasks with little or no fine-tuning. This is due to their extensive training, which allows them to generate more accurate and coherent responses in a variety of situations, including stance detection.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: add a table to illustrate the difference between BERT and LLMs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The role of fine-tuning\n",
    "\n",
    "While BERT is designed to be fine-tuned on tasks of interest like stance detection using labeled data, some of these more recent LLMs can perform stance detection without further fine-tuning with labeled data. \n",
    "\n",
    "Note that LLMs still undergo fine-tuning (see below for details), but they are not fine-tuned on a specific task like stance detection. Instead, they are fine-tuned on a general language modeling task, which allows them to learn the structure and style of human language. This general pre-training allows them to perform well on a variety of tasks, including stance detection, without further fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The Potential of Prompting\n",
    "\n",
    "While BERT is designed to be fine-tuned on specific tasks like stance detection using labeled data, some of these more recent LLMs can perform stance detection without fine-tuning, but with prompting techniques (i.e., the way you \"ask\" these models questions). These techniques involve providing the model with context or examples to guide its response, rather than relying on fine-tuning with labeled data.\n",
    "\n",
    "For example, with a zero-shot approach, an LLM can perform stance detection on tweets without being fine-tuned on a specific dataset. The model can understand the task and generate an appropriate response based on its extensive knowledge learned during pretraining."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: add a diagram to illustrate different prompting techniques]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two state-of-the-art LLMs: ChatGPT-3.5 and Flan-T5\n",
    "\n",
    "Large Language Models (LLMs) have made significant advancements in the field of natural language processing in recent years. These models excel at understanding and generating human-like text by learning from vast amounts of data. Two notable LLMs that are frequently used for tasks such as stance detection are flan-T5 and OpenAI's ChatGPT-3.5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### OpenAI's ChatGPT-3.5\n",
    "\n",
    "OpenAI's ChatGPT-3.5 is a state-of-the-art language model, known for its impressive capabilities in understanding context and generating coherent text. Like BERT, it is also based on the Transformer architecture, but it has a much larger number of parameters and has been trained on a much larger dataset.\n",
    "\n",
    "Due to the massive amount of training data, it perform well on various NLP tasks with little or no fine-tuning. ChatGPT-3.5's adaptability and powerful language processing make it a strong candidate for stance detection tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How ChatGPT-3.5 is trained?\n",
    "\n",
    "Its training process consists of two main steps: pre-training and fine-tuning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Pre-training in ChatGPT-3.5\n",
    "\n",
    "During the pre-training phase, ChatGPT-3.5 learns from vast amounts of text data gathered from various sources, such as websites, books, and articles. It is not explicitly trained on specific tasks at this stage; rather, it learns the structure, grammar, and general knowledge embedded in the text. The model's objective during pre-training is to predict the **next word** in a sentence, given the context of the words that come before it. This process, known as \"language modeling,\" helps the model develop a deep understanding of language patterns and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jalammar.github.io/how-gpt3-works-visualizations-animations/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gpt3_training.gif\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-tuning in ChatGPT-3.5\n",
    "\n",
    "In the fine-tuning process of ChatGPT-3.5, **reinforcement learning from human feedback** (RLHF) plays a crucial role. This approach involves creating an initial dataset with human AI trainers (AI trainers providing conversations or responses to various prompts), generating comparison data by ranking multiple model-generated responses, building a reward model based on these rankings, and optimizing the ChatGPT-3.5 model using the reward model. Iteratively incorporating human feedback helps improve the model's performance, making it produce responses that are more similar to human responses.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![chatgpt3-5](images/chatgpt_diagram.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Open-source model flan-t5\n",
    "\n",
    "The flan-T5 model is based on the T5 (Text-to-Text Transfer Transformer) architecture developed by Google Research. Like BERT and ChatGPT-3.5, flan-T5 is also built on the Transformer architecture. It is designed to handle a wide range of NLP tasks by converting them into a text-to-text format. The model's versatility and strong performance make it a popular choice for many short question-answering tasks, including stance detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add diagram to illustrate the T5 architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is flan-T5 trained?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Pre-training in flan-t5\n",
    "\n",
    "During the pre-training phase, flan-t5 undergoes a similar process to ChatGPT-3.5. Like ChatGPT3.5, it also learns from a large corpus of text data, which helps it develop a deep understanding of language patterns and relationships. The model's objective during pre-training is to predict the **next word** in a sentence, given the context of the words that come before it. This process, known as \"language modeling,\" helps the model develop a deep understanding of language patterns and relationships."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine-tuning in flan-T5\n",
    "\n",
    "For flan-T5, the fine-tuning process involves training the model on task-specific datasets. Unlike ChatGPT-3.5, which is focused on improving general conversational abilities, flan-T5 is designed to handle a wide range of NLP tasks by converting them into a text-to-text format. This means that flan-T5 is explicitly fine-tuned for various tasks like sentiment analysis, question answering, summarization, translation, and more.\n",
    "\n",
    "During fine-tuning, flan-T5 learns to generate appropriate output text corresponding to the input text and the specific task it is being fine-tuned for. For example, if flan-T5 is being fine-tuned for translation, it might receive an input text in English and generate the corresponding translated text in another language, like French or German.\n",
    "\n",
    "Some of the tasks that flan-T5 has been trained on include:\n",
    "\n",
    "- Sentiment Analysis: Determining the sentiment or emotion expressed in a given text, such as identifying whether a movie review is positive, negative, or neutral.\n",
    "\n",
    "- Named Entity Recognition (NER): Identifying and classifying entities such as names of people, organizations, locations, and dates within a text.\n",
    "\n",
    "- Translation: Translating text from one language to another, for example, converting English text to French or German."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flan-t5](images/flan_t5_xxl.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Distinctions between ChatGPT-3.5 and flan-T5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propietary vs. Open-source\n",
    "\n",
    "ChatGPT-3.5 is a proprietary model developed by OpenAI, and using it comes with costs, typically via API access. The implementation details and source code are not openly available, which limits users' ability to modify or understand the underlying workings of the model. On the other hand, flan-T5 is an open-source model, which means it is free to use, and the source code is publicly available for anyone to explore, use, and modify as needed. This distinction has implications for accessibility, transparency, and adaptability of the models for various tasks and research purposes.\n",
    "\n",
    "#### Training Objectives \n",
    "\n",
    "ChatGPT-3.5 focuses on improving general conversational abilities and controllability, whereas flan-T5 is designed to handle a wide-range of short question-answering NLP tasks. Therefore, flan-T5 may not be as effective as ChatGPT-3.5 for tasks that requires a longer response, such as writing an essay.\n",
    "\n",
    "#### Fine-tuning Data\n",
    "\n",
    "ChatGPT-3.5 is fine-tuned on a narrower dataset with demonstrations and comparisons with human AI trainers, while flan-T5 is fine-tuned on task-specific datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompting Techniques for LLMs\n",
    "\n",
    "To effectively utilize LLMs like flan-T5 and ChatGPT-3.5 for stance detection, we can employ various prompting techniques that guide the model's response without the need for fine-tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Zero-shot prompting\n",
    "\n",
    "In the zero-shot prompting technique, the LLM is provided with a task description and a sample input without any specific examples. The model uses its pre-trained knowledge to understand the task and generate an appropriate response.\n",
    "\n",
    "[TODO: add examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Few-shot prompting\n",
    "\n",
    "The few-shot prompting technique involves providing the LLM with a small number of examples to guide its response. This allows the model to learn from the provided examples and adapt its output accordingly.\n",
    "\n",
    "[TODO: add examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chain-of-thoughts method\n",
    "\n",
    "The chain-of-thoughts method involves breaking down a complex task into a series of simpler sub-tasks. The LLM generates intermediate outputs for each sub-task and uses these outputs as context to solve the overall problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO: add examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Programming Exercise: Implementing Stance Detection with flan-T5 and ChatGPT-3.5\n",
    "\n",
    "Now, let's implement stance detection using flan-t5 and GPT 3.5.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setting up the environment\n",
    "\n",
    "Install the necessary libraries and dependencies for flan-T5 and ChatGPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Set up the environment here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input prompts for each method\n",
    "\n",
    "Create input prompts for zero-shot, few-shot, and chain-of-thoughts methods, ensuring they are formatted correctly to guide the model's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare input prompts here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Implementing stance detection with flan-t5 and GPT 3.5\n",
    "\n",
    "Use the prepared input prompts with flan-T5 and ChatGPT-3.5 to perform stance detection on the given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement stance detection using flan-t5 and GPT 3.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing results and performance\n",
    "\n",
    "Analyze and compare the performance of flan-T5 and ChatGPT-3.5 for each prompting technique to determine their effectiveness in stance detection tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/Users/vimchiz/miniconda3/envs/gda/bin/python' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/vimchiz/miniconda3/envs/gda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare results and performance here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned how to perform stance detection on tweets using a fine-tuned BERT model and large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "435414adc4ce3b84c9fac3213024ef17f383b5213bd6fd3489d95b5052ab39f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
